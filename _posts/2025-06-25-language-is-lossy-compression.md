---
layout: post
title: "Language as a lossy compression algorithm"
date: 2025-06-25 14:15:00 +0100
author: Indigo Nolan
permalink: /blog/language-as-lossy-compression
tags: essay
---
In digital media, 'lossy compression' is the trade-off we make for convenience - we accept the loss of some data from an image to make it small enough to send. I've started to believe we perform this exact same operation on our thoughts every time we open our mouths. Every conversation is an exercise in unintended consequences. We send out carefully crafted sentences, only to find they arrive at their destination scrambled, with half the meaning lost. This isn't a personal failure, but a fundamental limitation of language itself.

That is, language is essentially a lossy compression algorithm.

## What Do You Mean, Lossy?

Think of your thoughts as a high-quality, complex, multi-sensory experience in your brain. It's a collection of feelings, logic, and rationale. It's an 8K, 120fps video with a full orchestral score behind it.

But you can't transmit this thought in all its glory to the person you're speaking to. That would take hours, and you probably don't want them knowing every step you took to finalise the thought anyway. So, you choose your words carefully to find the best way to express yourself - to transmit the high-quality video file.

You choose your vocabulary, your grammar, and which information to drop for the sake of conciseness or convenience. But what information is lost in the compression? Some emotional nuance, sensory detail, underlying context, tone?

When the person listening hears what you say, they have to 'decompress' it to interpret what it all means. The problem lies here - they have their own algorithm. They don't share your experiences, your mood, and your intention. They will deconstruct what you have said based on their own worldview. The final image they recreate in their head will never be identical to your original.

So, what happens when the listeners 'decompression algorithm' goes wrong? This is where cognitive bias comes in. If a listener has a biased worldview (which we all do in our own way), they will be actively looking for data that confirms their existing beliefs and discard the rest, leading to a completely different recreated image than the one the speaker intended. Two people can even listen to the same speech and come to two entirely different conclusions - because of their pre-existing conceptions and biases, and because of how they deconstructed the language of the speech in their mind.


All of this is only worsened if the conversation is taking place by text message, or email. The algorithm becomes aggressively lossy. Facial expressions, tonal shifts in your voice, pacing of words - these are all lost to the void when you are writing messages out on your phone's keyboard.

Think about the last time you tried to make a joke in a group chat. Assuming you didn't use [tone indicators](https://toneindicators.carrd.co/), how did you convey that it was a joke? Or sarcastic? Or meant to be taken lightly? Presumably, you assumed the recipients would infer that from context. This may usually work, but when cultures clash, or generations have different expectations of humour, this can result in awkward misunderstandings. Modern communication has attempted to re-introduce emotions and tone to language via emojis. Sending a smiley face :) or a laughing emoji üòÇ is a tiny packet of information that replaces the expected body language or facial expressions that were intended to go along with the message sent.

Have you ever received (or sent) an "Okay." text? Or a "Noted." email? Have you ever thought about the emotional impact that the final full stop in that message can have? Lots of people won't give it a second thought, but many more people will take it to heart as a personal attack, an expression of disinterest or muted anger. This is the subtle nuance we lose without the metadata of tone and body language that is carried when communicating in real life.

Translating between languages is equally a challenge. Words like the German `Schadenfreude` (joy from witnessing others' failures) or the Japanese `Komorebi` (sunlight filtering through trees) are difficult to translate succintly into English. They are attempts to package complex concepts and feelings into one word.

How about describing a dream? Dreams are an excellent example of a high-fidelty internal experience that inevitably becomes absurd and nonsensical when you try to compress it into communictable language. You were falling, but then you weren't, but it all felt real? You can't convey that via speech, you can't express how you physically felt at the time - we don't have all the words to describe the human experience. Not yet.

## Difference Languages, Different Codecs?

This idea, that the specific tools out language gives us can shape what we are able to express, relates to a fascinating linguistic concept and personal favourite of mine - [the Sapir-Whorf hypothesis, commonly known as linguistic relativity](https://en.wikipedia.org/wiki/Linguistic_relativity). In short, it suggests that the 'compression algorithm' we are given (our native language), doesn't simply convey our thoughts - it can actually shape and limit the thoughts we're able to have in the first place. This guarantees a certain amount of 'loss' when communicating between different linguistic systems - if English doesn't have a word for a concept, do English speakers simply encounter that concept less often?

This idea has, historically, been used to supress native languages, especially Native American languages - with the idea their speakers would 'be better off learning English' as a more 'civilised' way of thinking. I, of course, reject this - many native languages are much richer than English, and we have much to learn by learning about them.

The commonly cited claim that Inuit and Yupik indigenous people have [hundreds of words for snow](https://en.wikipedia.org/wiki/Eskimo_words_for_snow) is actually a persistent myth that has been thoroughly debunked by linguists. In reality, these languages have roughly the same number of basic snow terms as English does. However, the myth does point to something interesting about polysynthetic languages - they can create very long and complex words by adding different suffixes to a root word to describe a situation with incredible precision.

Ironically, English speakers do something remarkably similar, we just don't think of it as "one word." Consider the phrase: "dress you inherited from your grandma day." Is it 'grandma day'? No. Is your grandma named 'day'? No. Every word in that sentence except "day" is essentially a modifier onto the word "day". It is 'the day of the dress that you inherited from your grandma' - we're building up complex meaning by stacking descriptive elements together, much like polysynthetic languages do within single words. The difference is largely orthographic (how we write it) rather than conceptual.

What is true is that languages like Inuktitut are polysynthetic, meaning they can pack what English expresses in entire phrases into single words. But the idea that they have vastly more snow vocabulary than English is simply false - we have powder, slush, sleet, frost, flurries, blizzard conditions, and many other terms too.

To see some real examples of lossiness in action, here are quite a few concepts from around the world that English can struggle to compress without data loss:

*   `Piliriqatigiinniq` (from Inuktitut), which means 'working together for a common cause'.
*   `Dadirri` (from the Australian language Ngan'gikurunggurr), which roughly means the concept of 'deep listening' or 'reflection'.
*   `H√≥zh√≥` (from Navajo), which roughly means the concept of 'walking peacefully' or 'walking in beauty'.
*   `Mamihlapinatapai` (from the South American language Yaghan), which roughly means the situation when multiple people both want something to happen, but neither want to start it.
*   `Kanyininpa` (from the Australian language Pintupi), which roughly means 'holding' or 'caring', but it carries a stronger meaning, like the deep, nurturing feeling from a parent to child.
*   `Murr-ma` (from the Australian language Wagiman), which is the very specific act of searching for something in water with only your feet.
*   `T≈´rangawaewae` (from MƒÅori), which roughly means 'a place to stand', but moreso a 'hometown where you feel like you belong'.
*   `Mino-bimaadiziwin` (from the North American language Anishinaabe), which is the holistic concept of a 'good life' or a life well-lived.
*   `Sumak Kawsay` (from Quechua), which roughly means 'good living' or 'a plentiful life'. This became so important in South America that it [has been codified into the constitutions of Ecuador and Bolivia](https://en.wikipedia.org/wiki/Sumak_kawsay).

When translating these words across different languages, here is where our 'lossy compression' analogy comes into play - a bad translation can simplify meaning, remove nuance, and result in misunderstanding. There are so many situations (searching for something in water with only your feet comes to mind) that English speakers simply did not encounter frequently enough to need one succint word for.

There is also the idea of [high-context and low-context language cultures](https://www.ebsco.com/research-starters/communication-and-mass-media/high-context-and-low-context-cultures). European and American cultures are often described as low-context cultures - they expect communication to be explicit and direction. They try to cram as much data as possible into the compressed file, because the individual receiving it is expecting that. In contrast, cultures such as African and Arab and many Asian countries, rely more heavily on shared context, non-verbal clues and relationship backgrounds. The actual words may only form a small part of the message - it is assumed that both parties share a lot of 'metadata' about the conversation already, so the file can be smaller and still be understood.

A businessman in a high-context culture might find it desirable to create a working relationship with a customer before attempting to sell them something. A customer in a low-context culture might find this intimidating or unusual, in an environment where privacy and individuality are more valued.

This is of course a simplification, and no culture can be entirely 'high' or 'low' context - every person is different. But it is nonetheless interesting - when you communicate with your family, you will switch to a 'high context' environment, where you can relax and speak in shorter sentences, with the comfort that your conversation partners share lots of context with you. However, when at work, or in a business environment with strangers, you may switch to low-context - intentionally over-communicate, or spell things out, to prevent miscommunication between someone that you don't expect to understand your nuance or subtleties.

## When Is It Useful?

This incredibly advanced capability of human language is not all bad. We are very good at efficiently utilising 'codecs' - shortcuts to convert data streams into simpler, more understable ones. If you were to write down the tone of voice, the facial expressions, the body language - and deliver this packaged with the actual content of the message - this would likely overwhelm us. But when we are talking to someone in person, we all do this daily and subconsciously, adapting our behaviour and our interpretation of what we are being told, because of all those factors.

When you have a deep connection or a shared history with someone, like a partner, best friend, or sibling, you develop a highly efficient, personalised, and partially shared compression system. An inside joke, for example, is a tiny packet of information that can decompress into a huge, shared memory - while not making sense to an outsider.

As we grow, so does our compression algorithm - a trained lawyer will be much more proficient at summarising legal briefs (without losing key data!) than a person on the street. We tweak our algorithms to filter out all unnecessary information, and at the same time keep the important information. This is exactly how lossy algorithms work in digital media, too.


We need to realise that communication is inherently flawed. Asking clarifying questions is never a bad idea - "what I'm getting from that is..." can help us be more patient listeners, and prevent all sorts of misunderstandings before your data integrity is entirely compromised. Active listening is a form of 'error-correction' - by intentionally trying to understand the original intent of the person, you can simulate a [TCP confirmation handshake](https://www.geeksforgeeks.org/computer-networks/tcp-3-way-handshake-process/) - you are checking that what you received is the same as what they intended to send.

Ultimately, I believe seeing language this way is a step towards a more empathetic society. We all need to try our best to understand the other person - and we all need to try to communicate cooperatively, hopefully without [losing too much of our original intent.](https://en.wikipedia.org/wiki/Signal-to-noise_ratio)